{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "if major_version >= 8:\n",
    "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
    "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "else:\n",
    "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
    "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "pass\n",
    "!pip install triton transformers\n",
    "!pip install -U datasets\n",
    "!pip install --pre -U xformers ##### this take some time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ****Note**: Restart the Kernel after package installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T09:58:51.489978Z",
     "iopub.status.busy": "2025-04-22T09:58:51.489685Z",
     "iopub.status.idle": "2025-04-22T09:58:51.663087Z",
     "shell.execute_reply": "2025-04-22T09:58:51.662067Z",
     "shell.execute_reply.started": "2025-04-22T09:58:51.489946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"hf_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T09:58:53.086051Z",
     "iopub.status.busy": "2025-04-22T09:58:53.085724Z",
     "iopub.status.idle": "2025-04-22T09:59:16.265577Z",
     "shell.execute_reply": "2025-04-22T09:59:16.264621Z",
     "shell.execute_reply.started": "2025-04-22T09:58:53.086026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 09:59:01.153167: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-22 09:59:01.153283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-22 09:59:01.300116: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from IPython.display import display_markdown\n",
    "max_seq_length = 512 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",  \n",
    "]  #### loadin llama 3 model in 4 bit to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:03:32.638423Z",
     "iopub.status.busy": "2025-04-22T10:03:32.637778Z",
     "iopub.status.idle": "2025-04-22T10:03:40.858664Z",
     "shell.execute_reply": "2025-04-22T10:03:40.857670Z",
     "shell.execute_reply.started": "2025-04-22T10:03:32.638395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30.dev1005. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:03:40.860984Z",
     "iopub.status.busy": "2025-04-22T10:03:40.860616Z",
     "iopub.status.idle": "2025-04-22T10:03:47.882553Z",
     "shell.execute_reply": "2025-04-22T10:03:47.881465Z",
     "shell.execute_reply.started": "2025-04-22T10:03:40.860951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T09:59:49.119711Z",
     "iopub.status.busy": "2025-04-22T09:59:49.119368Z",
     "iopub.status.idle": "2025-04-22T10:00:23.858696Z",
     "shell.execute_reply": "2025-04-22T10:00:23.857336Z",
     "shell.execute_reply.started": "2025-04-22T09:59:49.119687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U sentencepiece\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:00:23.860646Z",
     "iopub.status.busy": "2025-04-22T10:00:23.860326Z",
     "iopub.status.idle": "2025-04-22T10:00:24.243967Z",
     "shell.execute_reply": "2025-04-22T10:00:24.242982Z",
     "shell.execute_reply.started": "2025-04-22T10:00:23.860620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21801 2423 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "import sacrebleu\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "with open('/kaggle/input/train-eng-bdq-phase2dataset-v1/bdq-eng.train.bdq', 'r', encoding='utf-8') as file_ba:\n",
    "    ba_data = file_ba.readlines()\n",
    "with open('/kaggle/input/train-eng-bdq-phase2dataset-v1/bdq-eng.train.eng', 'r', encoding='utf-8') as file_en:\n",
    "    en_data = file_en.readlines()\n",
    "assert len(ba_data) == len(en_data), \"The files don't have the same number of lines.\"\n",
    "train_df = pd.DataFrame({'English': en_data, 'Bahnar': ba_data})\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "\n",
    "ba_en_train_dataset = Dataset.from_pandas(train_df)\n",
    "ba_en_val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "with open('/kaggle/input/test-eng-bdq-phase2dataset-v1/bdq-eng.test.bdq', 'r', encoding='utf-8') as file_ba:\n",
    "    ba_data = file_ba.readlines()\n",
    "with open('/kaggle/input/test-eng-bdq-phase2dataset-v1/bdq-eng.test.eng', 'r', encoding='utf-8') as file_en:\n",
    "    en_data = file_en.readlines()\n",
    "assert len(ba_data) == len(en_data), \"The files don't have the same number of lines.\"\n",
    "test_df = pd.DataFrame({'English': en_data, 'Bahnar': ba_data})\n",
    "ba_en_test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(len(ba_en_train_dataset), len(ba_en_val_dataset), len(ba_en_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map each sample to the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:00:24.246094Z",
     "iopub.status.busy": "2025-04-22T10:00:24.245267Z",
     "iopub.status.idle": "2025-04-22T10:00:24.251406Z",
     "shell.execute_reply": "2025-04-22T10:00:24.250318Z",
     "shell.execute_reply.started": "2025-04-22T10:00:24.246060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Must add EOS_TOKEN at response last line\n",
    "EOS_TOKEN = tokenizer.eos_token \n",
    "\n",
    "# EOS_TOKEN = \"PIKACHU\"\n",
    "def mapping_response(sample):\n",
    "    sample['text'] = \"You are an expert Bahnar translator! Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \\n Translate the Bahnar input text into English. \\n\"+sample['Bahnar']+\"\\n ### Response: \\n\"+sample['English']+EOS_TOKEN\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:00:24.252662Z",
     "iopub.status.busy": "2025-04-22T10:00:24.252390Z",
     "iopub.status.idle": "2025-04-22T10:00:25.828702Z",
     "shell.execute_reply": "2025-04-22T10:00:25.827788Z",
     "shell.execute_reply.started": "2025-04-22T10:00:24.252636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bba3fb6b300482388bfa2547f0a2d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b3e40c014a48d38ff50388c7a5f557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ba_en_train_dataset = ba_en_train_dataset.map(mapping_response)\n",
    "ba_en_val_dataset = ba_en_val_dataset.map(mapping_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:00:25.830163Z",
     "iopub.status.busy": "2025-04-22T10:00:25.829904Z",
     "iopub.status.idle": "2025-04-22T10:00:25.905992Z",
     "shell.execute_reply": "2025-04-22T10:00:25.905058Z",
     "shell.execute_reply.started": "2025-04-22T10:00:25.830142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8873d4587e064ff185c4856d4fb93caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ba_en_test_dataset = ba_en_test_dataset.map(mapping_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Select 32 good entries from the test dataset for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:00:25.908000Z",
     "iopub.status.busy": "2025-04-22T10:00:25.907312Z",
     "iopub.status.idle": "2025-04-22T10:00:25.917351Z",
     "shell.execute_reply": "2025-04-22T10:00:25.916541Z",
     "shell.execute_reply.started": "2025-04-22T10:00:25.907964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "good_entries = [5, 99, 96, 89, 87, 84, 83, 77, 64, 65, 66, 61, 58, 46, 42, 49, 48, 47, 26, 23, 100, 107, 109, 112, 141, 175, 176, 177, 196, 198, 171, 157]\n",
    "good_32 = ba_en_test_dataset.select(good_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:00:25.919942Z",
     "iopub.status.busy": "2025-04-22T10:00:25.919680Z",
     "iopub.status.idle": "2025-04-22T10:00:25.931536Z",
     "shell.execute_reply": "2025-04-22T10:00:25.930769Z",
     "shell.execute_reply.started": "2025-04-22T10:00:25.919921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prompt_inference(prmpt):\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prmpt\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 512, temperature=0.1, use_cache = True)\n",
    "    return tokenizer.batch_decode(outputs)[0].split(\"### Response:\")[-1].split(EOS_TOKEN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:00:25.933520Z",
     "iopub.status.busy": "2025-04-22T10:00:25.932779Z",
     "iopub.status.idle": "2025-04-22T10:00:25.948434Z",
     "shell.execute_reply": "2025-04-22T10:00:25.947459Z",
     "shell.execute_reply.started": "2025-04-22T10:00:25.933487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert Bahnar translator! Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \n",
      " Translate the Bahnar input text into English. \n",
      "Thoi noh kăl khŏm mă ming hơmet rai hrôih rai 'lơ̆ng.\n",
      "\n",
      " ### Response: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are an expert Bahnar translator! Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \\n Translate the Bahnar input text into English. \\n\" + ba_en_test_dataset['Bahnar'][90] +\"\\n ### Response: \\n\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:00:25.950214Z",
     "iopub.status.busy": "2025-04-22T10:00:25.949859Z",
     "iopub.status.idle": "2025-04-22T10:00:59.486946Z",
     "shell.execute_reply": "2025-04-22T10:00:59.485949Z",
     "shell.execute_reply.started": "2025-04-22T10:00:25.950183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       " The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated into English as follows: \n",
       "The Bahnar input text is translated"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"result\")\n",
    "display_markdown(prompt_inference(prmpt=prompt),raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:04:09.587113Z",
     "iopub.status.busy": "2025-04-22T10:04:09.586748Z",
     "iopub.status.idle": "2025-04-22T10:04:11.764881Z",
     "shell.execute_reply": "2025-04-22T10:04:11.763604Z",
     "shell.execute_reply.started": "2025-04-22T10:04:09.587087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a054ccbfa51476ebbfa5b7616ed1d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = good_32,\n",
    "    dataset_text_field = \"text\", ### taking text column from the dataset\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        report_to=None,\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 500,\n",
    "        logging_steps=1000,\n",
    "        learning_rate = 2e-4,\n",
    "        num_train_epochs=5,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:04:17.568555Z",
     "iopub.status.busy": "2025-04-22T10:04:17.567814Z",
     "iopub.status.idle": "2025-04-22T10:09:08.928155Z",
     "shell.execute_reply": "2025-04-22T10:09:08.927285Z",
     "shell.execute_reply.started": "2025-04-22T10:04:17.568525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 32 | Num Epochs = 5 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 04:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=3.373370361328125, metrics={'train_runtime': 289.0228, 'train_samples_per_second': 0.554, 'train_steps_per_second': 0.277, 'total_flos': 1161689346908160.0, 'train_loss': 3.373370361328125})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:09:28.888515Z",
     "iopub.status.busy": "2025-04-22T10:09:28.887869Z",
     "iopub.status.idle": "2025-04-22T10:09:28.933677Z",
     "shell.execute_reply": "2025-04-22T10:09:28.932674Z",
     "shell.execute_reply.started": "2025-04-22T10:09:28.888478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert Bahnar translator! Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \n",
      " Translate the Bahnar input text into English. \n",
      "Sôlômôn hơ-oei kơpal tang-dŏ pơtao Đawit 'bă sư păng dêh char pơtao sư kơjăp 'lơ̆ng tơpă\n",
      "\n",
      " ### Response: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are an expert Bahnar translator! Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \\n Translate the Bahnar input text into English. \\n\" + ba_en_train_dataset['Bahnar'][94] +\"\\n ### Response: \\n\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:09:32.035557Z",
     "iopub.status.busy": "2025-04-22T10:09:32.034746Z",
     "iopub.status.idle": "2025-04-22T10:09:33.222374Z",
     "shell.execute_reply": "2025-04-22T10:09:33.221311Z",
     "shell.execute_reply.started": "2025-04-22T10:09:32.035532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       "So, I think that's a good place to start.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"result\")\n",
    "display_markdown(prompt_inference(prmpt=prompt),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:09:37.614339Z",
     "iopub.status.busy": "2025-04-22T10:09:37.613927Z",
     "iopub.status.idle": "2025-04-22T10:09:37.665911Z",
     "shell.execute_reply": "2025-04-22T10:09:37.664943Z",
     "shell.execute_reply.started": "2025-04-22T10:09:37.614312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an expert Bahnar translator! Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \\n Translate the Bahnar input text into English. \\nLu sư kư̆m pơ̆ng pơlôch 'bar 'nu bơngai tơtông jê̆ hơdai Yêsu, minh 'nu gah 'ma, minh 'nu gah 'ngiĕo kơ Sư\\n\\n ### Response: \\nThen were there two thieves crucified with him, one on the right hand, and another on the left\\n<|end_of_text|>\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ba_en_train_dataset['text'][90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the whole test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:09:59.888944Z",
     "iopub.status.busy": "2025-04-22T10:09:59.888567Z",
     "iopub.status.idle": "2025-04-22T10:57:18.624878Z",
     "shell.execute_reply": "2025-04-22T10:57:18.623747Z",
     "shell.execute_reply.started": "2025-04-22T10:09:59.888920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from progress.bar import Bar\n",
    "\n",
    "def evaluate_model(model, encode_tokenizer, decode_tokenizer, dataset, device):\n",
    "    model.eval()\n",
    "    prt = True\n",
    "\n",
    "    predictions, references = [], []\n",
    "    for i in range(len(dataset)):\n",
    "            print(i)\n",
    "            example = dataset[i]\n",
    "            prmpt = \"You are an expert Bahnar translator! Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \\n Translate the Bahnar input text into English. \\n\" + example['Bahnar'] +\"\\n ### Response: \\n\"\n",
    "            FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "            inputs = encode_tokenizer(\n",
    "            [\n",
    "                prmpt\n",
    "            ], return_tensors = \"pt\").to(\"cuda\")\n",
    "            \n",
    "            outputs = model.generate(**inputs, max_new_tokens = 512, temperature=0.1, use_cache = True)\n",
    "            pred = decode_tokenizer.batch_decode(outputs)[0].split(\"### Response:\")[-1].split(\"EOS_TOKEN\")[0]\n",
    "            predictions.append([pred])\n",
    "    \n",
    "            references.append([example['English']])\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "test_examples = [{'Bahnar': ex['Bahnar'], 'English': ex['English']} for ex in ba_en_test_dataset]\n",
    "predictions, references = evaluate_model(model, tokenizer, tokenizer, test_examples, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:57:24.297067Z",
     "iopub.status.busy": "2025-04-22T10:57:24.296483Z",
     "iopub.status.idle": "2025-04-22T10:57:24.312342Z",
     "shell.execute_reply": "2025-04-22T10:57:24.311095Z",
     "shell.execute_reply.started": "2025-04-22T10:57:24.297019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' \\nAnd then, the next day, we went to the beach.\\n<|end_of_text|>'],\n",
       " ['I release you, but can do no more.\\n'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0], references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:57:33.018741Z",
     "iopub.status.busy": "2025-04-22T10:57:33.017925Z",
     "iopub.status.idle": "2025-04-22T10:57:33.027122Z",
     "shell.execute_reply": "2025-04-22T10:57:33.025762Z",
     "shell.execute_reply.started": "2025-04-22T10:57:33.018702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = [text[0].split(EOS_TOKEN)[0].strip() for text in predictions]\n",
    "refs = [text[0].strip() for text in references]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:57:37.100435Z",
     "iopub.status.busy": "2025-04-22T10:57:37.099531Z",
     "iopub.status.idle": "2025-04-22T10:57:37.796168Z",
     "shell.execute_reply": "2025-04-22T10:57:37.795234Z",
     "shell.execute_reply.started": "2025-04-22T10:57:37.100406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU:  0.82\n",
      "CHRF: 13.59\n",
      "TER: 979.56\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
    "print(\"BLEU: \", round(bleu.score, 2))\n",
    "\n",
    "# Calculate CHRF\n",
    "chrf = sacrebleu.corpus_chrf(preds, refs)\n",
    "print(\"CHRF:\", round(chrf.score, 2))\n",
    "\n",
    "# Calculate TER\n",
    "metric = sacrebleu.metrics.TER()\n",
    "ter = metric.corpus_score(preds, refs)\n",
    "print(\"TER:\", round(ter.score, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:57:52.918133Z",
     "iopub.status.busy": "2025-04-22T10:57:52.917410Z",
     "iopub.status.idle": "2025-04-22T10:57:52.932723Z",
     "shell.execute_reply": "2025-04-22T10:57:52.931538Z",
     "shell.execute_reply.started": "2025-04-22T10:57:52.918102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: And then, the next day, we went to the beach.\n",
      "Reference 1: I release you, but can do no more.\n",
      "Prediction 2: And they will be able to do that in the future.\n",
      "Reference 2: You have as much right to be a knight as any man.\n",
      "Prediction 3: And I'm not sure.\n",
      "Reference 3: No. Leave me alone.\n",
      "Prediction 4: I don't know.\n",
      "Reference 4: I'm here to help!\n",
      "Prediction 5: If you don't know, ask.\n",
      "Reference 5: All you wanna do is pee.\n",
      "Prediction 6: And now we see the same thing happening in Tunisia and Egypt, where the people are rising up and demanding change.\n",
      "Reference 6: And these platforms were certainly very helpful to activists in Tunisia and Egypt this past spring and beyond.\n",
      "Prediction 7: And then Charlie said, \"I'm not sure I understand what you're saying.\"\n",
      "Reference 7: He's an architect, and Charlie is deeply concerned about global climate change.\n",
      "Prediction 8: And they will be able to do that.\n",
      "Reference 8: She took my little girl.\n",
      "Prediction 9: Oh, I don't think I can do that.\n",
      "Reference 9: No, please, it's not my freedom I seek.\n",
      "Prediction 10: But why?\n",
      "Reference 10: After all, who am I?\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Prediction {i+1}: {preds[i]}\")\n",
    "    print(f\"Reference {i+1}: {refs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"tuongdc03/model_name\", token = hf_token)\n",
    "tokenizer.push_to_hub(\"tuongdc03/model_name\", token = hf_token)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5063149,
     "sourceId": 8487291,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5063775,
     "sourceId": 8488207,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
